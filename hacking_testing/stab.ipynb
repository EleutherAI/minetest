{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7120aaa-4083-4b42-9ef9-696782ebd1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install stable-baselines3\n",
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30299731-ef99-4e99-b556-7bbfa362f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vpt-minetest' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rk1a/vpt-minetest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d03423d-4f8c-41c7-a51c-4027d64e4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!wget -c https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-2x.weights\n",
    "!wget -c https://openaipublic.blob.core.windows.net/minecraft-rl/models/2x.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "279cd3e2-3402-4cd0-b23a-102f032a8271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stab.py\n"
     ]
    }
   ],
   "source": [
    "%%file stab.py\n",
    "import pickle\n",
    "import numpy as np\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from gym.spaces import Dict as DictOAI, Box, Discrete, MultiDiscrete\n",
    "from gym.wrappers import Monitor, TimeLimit\n",
    "from minetest_env import Minetest\n",
    "from stable_baselines3 import PPO\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from stable_baselines3.common.distributions import (\n",
    "    Distribution,\n",
    "    MultiCategoricalDistribution,\n",
    ")\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from gym_wrappers import (DictToMultiDiscreteActionSpace,\n",
    "                          HiddenStateObservationSpace, ObservationToCPU,\n",
    "                          ObservationToInfos)\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "import torch\n",
    "import gym\n",
    "import gc\n",
    "import sys\n",
    "if \"./vpt-minetest\" not in sys.path:\n",
    "    sys.path.append(\"./vpt-minetest\")\n",
    "from agent import MineRLAgent\n",
    "from run_vpt_agent import minetest_to_minerl_obs, minerl_to_minetest_action\n",
    "\n",
    "\n",
    "model, weights, video_dir, minetest_path, max_steps, show, seed, show_agent_pov = \"2x.model\", \"foundation-model-2x.weights\", \"videos\", \"../bin/minetest\", 100000, False, 32, False\n",
    "\n",
    "\n",
    "# class DiscreteActions(gym.ActionWrapper):\n",
    "#     def __init__(self, env, discretes=64):\n",
    "#         self.env = env\n",
    "#         self.discretes = discretes\n",
    "#         sizes = []\n",
    "#         self.vals = []\n",
    "#         for i, v in env.action_space.spaces.items():\n",
    "#             self.vals.append(len(sizes))\n",
    "#             if isinstance(v, Discrete):\n",
    "#                 sizes.append(v.n)\n",
    "#             elif isinstance(v, Box):\n",
    "#                 for _ in v.low:\n",
    "#                     sizes.append(discretes)\n",
    "                \n",
    "#         self.action_space = MultiDiscrete(sizes)  # TODO\n",
    "    \n",
    "#     def action(self, act):\n",
    "#         return {k: (np.asarray(act[i:i+len(v.low)]) / self.discretes * (v.high - v.low) + v.low\n",
    "#                     if isinstance(v, Box) else act[i]).astype(v.dtype)\n",
    "#                 for i, (k, v) in zip(self.vals, self.env.action_space.spaces.items())}\n",
    "\n",
    "def _env_action_to_agent(self, minerl_action_transformed, action_transformer, action_mapper, to_torch=False, check_if_null=False):\n",
    "    \"\"\"\n",
    "    Turn action from MineRL to model's action.\n",
    "\n",
    "    Note that this will add batch dimensions to the action.\n",
    "    Returns numpy arrays, unless `to_torch` is True, in which case it returns torch tensors.\n",
    "\n",
    "    If `check_if_null` is True, check if the action is null (no action) after the initial\n",
    "    transformation. This matches the behaviour done in OpenAI's VPT work.\n",
    "    If action is null, return \"None\" instead\n",
    "    \"\"\"\n",
    "    minerl_action = action_transformer.env2policy(minerl_action_transformed)\n",
    "    if check_if_null:\n",
    "        if np.all(minerl_action[\"buttons\"] == 0) and np.all(minerl_action[\"camera\"] == action_transformer.camera_zero_bin):\n",
    "            return None\n",
    "\n",
    "    # Add batch dims if not existant\n",
    "    if minerl_action[\"camera\"].ndim == 1:\n",
    "        minerl_action = {k: v[None] for k, v in minerl_action.items()}\n",
    "    action = action_mapper.from_factored(minerl_action)\n",
    "    if to_torch:\n",
    "        action = {k: th.from_numpy(v).to(self.device) for k, v in action.items()}\n",
    "    return action\n",
    "\n",
    "class DictToMultiDiscreteActionSpace(gym.Wrapper):\n",
    "    \"\"\"Converts Dict to MultiDiscrete action space for MineRL envs\"\"\"\n",
    "\n",
    "    def __init__(self, env, action_transformer, action_mapper):\n",
    "        super().__init__(env)\n",
    "        print(self.env.action_space)\n",
    "\n",
    "        if not isinstance(self.env.action_space, DictOAI):\n",
    "            raise ValueError(\"Original action space is not of type gym.Dict.\")\n",
    "\n",
    "        # assert minerl_agent is not None\n",
    "\n",
    "        # self.minerl_agent = minerl_agent\n",
    "        self.action_transformer = action_transformer\n",
    "        self.action_mapper = action_mapper\n",
    "\n",
    "        # first dimension = camera, second dimension = buttons\n",
    "        self.action_space = MultiDiscrete([121, 8641])\n",
    "\n",
    "        # check action space conversion\n",
    "        # random_env_action = self.env.action_space.sample()\n",
    "        # agent_action = self.minerl_agent._env_action_to_agent(random_env_action)\n",
    "        # array_action = self.to_array_action(agent_action)\n",
    "        # assert array_action.squeeze() in self.action_space\n",
    "\n",
    "        # agent_action2 = self.to_agent_action(array_action)\n",
    "        # env_action = self.minerl_agent._agent_action_to_env(agent_action2)\n",
    "        # env_action[\"ESC\"] = env_action[\"swapHands\"] = env_action[\"pickItem\"] = np.array(\n",
    "            # 0\n",
    "        # )\n",
    "        # assert env_action in self.env.action_space\n",
    "\n",
    "    def to_array_action(self, agent_action):\n",
    "        if isinstance(agent_action[\"camera\"], np.ndarray):\n",
    "            array_action = np.concatenate(\n",
    "                (agent_action[\"camera\"], agent_action[\"buttons\"]), -1\n",
    "            )\n",
    "        elif isinstance(agent_action[\"camera\"], th.Tensor):\n",
    "            array_action = th.cat(\n",
    "                (agent_action[\"camera\"], agent_action[\"buttons\"]), dim=-1\n",
    "            )\n",
    "        return array_action\n",
    "\n",
    "    def to_agent_action(self, array_action):\n",
    "        agent_action = {\"camera\": array_action[..., 0], \"buttons\": array_action[..., 1]}\n",
    "        return agent_action\n",
    "\n",
    "    def step(self, action):\n",
    "        if len(action.shape) < 2:\n",
    "            action = action[np.newaxis, :]\n",
    "        # transform array action to agent action to MineRL action\n",
    "        agent_action = self.to_agent_action(action)\n",
    "        minerl_action = self.minerl_agent._agent_action_to_env(agent_action)\n",
    "\n",
    "        # TODO implement policy that controls the remaining actions (especially ESC):\n",
    "        minerl_action[\"ESC\"] = np.array(0)\n",
    "        minerl_action[\"swapHands\"] = minerl_action[\"pickItem\"] = np.array(0)\n",
    "\n",
    "        obs, reward, terminated, info = self.env.step(minerl_action)\n",
    "\n",
    "        return obs, reward, terminated, info\n",
    "\n",
    "def make_env(\n",
    "    minetest_path: str,\n",
    "    rank: int,\n",
    "    seed: int = 0,\n",
    "    max_steps: int = 1e9,\n",
    "    env_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    action_transformer=None, action_mapper=None\n",
    "):\n",
    "    env_kwargs = env_kwargs or {}\n",
    "\n",
    "    def _init():\n",
    "        # Make sure that each Minetest instance has\n",
    "        # - different server and client ports\n",
    "        # - different and deterministic seeds\n",
    "        env = Minetest(\n",
    "            env_port=5555 + rank,\n",
    "            server_port=30000 + rank,\n",
    "            # seed=seed + rank,\n",
    "            world_dir=f\"../worlds/myworld{rank}\",\n",
    "            minetest_executable=minetest_path,\n",
    "            # xvfb_headless=False,\n",
    "            config_path=\"../minetest.conf\",\n",
    "            **env_kwargs,\n",
    "        )\n",
    "        env.reset_world = True\n",
    "        env = TimeLimit(env, max_episode_steps=max_steps)\n",
    "        # env = DiscreteActions(env)\n",
    "        # env = ObservationToInfos(env)\n",
    "        env = DictToMultiDiscreteActionSpace(env, \n",
    "            action_transformer=action_transformer,\n",
    "            action_mapper=action_mapper,)\n",
    "        # env = HiddenStateObservationSpace(env, minerl_agent)\n",
    "        # env = ObservationToCPU(env)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "# Env settings\n",
    "seed = 42\n",
    "max_steps = 4\n",
    "env_kwargs = {\"display_size\": (1024, 600), \"fov\": 72}\n",
    "\n",
    "print(\"---Loading model---\")\n",
    "agent_parameters = pickle.load(open(model, \"rb\"))\n",
    "policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "agent = MineRLAgent(\n",
    "    None,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    pi_head_kwargs=pi_head_kwargs,\n",
    "    show_agent_perspective=show_agent_pov,\n",
    ")\n",
    "# agent.load_weights(weights)\n",
    "agent_kwargs = dict(\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    pi_head_kwargs=pi_head_kwargs,\n",
    "    show_agent_pov=show_agent_pov,\n",
    ")\n",
    "\n",
    "# Create a vectorized environment\n",
    "num_envs = 2  # Number of envs to use (<= number of avail. cpus)\n",
    "# vec_env_cls = SubprocVecEnv\n",
    "vec_env_cls = DummyVecEnv\n",
    "venv = vec_env_cls(\n",
    "    [\n",
    "        make_env(minetest_path=minetest_path, rank=i, seed=seed, max_steps=max_steps, env_kwargs=env_kwargs,\n",
    "                 action_transformer=agent.action_transformer, action_mapper=agent.action_mapper)\n",
    "        for i in range(num_envs)\n",
    "    ],\n",
    ")\n",
    "\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "class MinecraftActorCriticPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy class for actor-critic algorithms wrapping OpenAI's VPT models.\n",
    "    Used by A2C, PPO and the likes.\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param minerl_agent: MineRL agent to be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        minerl_agent: MineRLAgent,\n",
    "        # policy_kwargs: Dict = {},\n",
    "        # pi_head_kwargs: Dict = {},\n",
    "        # show_agent_pov: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        # self.minerl_agent = minerl_agent\n",
    "        # print(policy_kwargs)\n",
    "        self.minerl_agent = minerl_agent\n",
    "        # self.minerl_agent = MineRLAgent(\n",
    "        #     None,\n",
    "        #     # venv,\n",
    "        #     policy_kwargs=policy_kwargs,\n",
    "        #     pi_head_kwargs=pi_head_kwargs,\n",
    "        #     show_agent_perspective=show_agent_pov,\n",
    "        # )\n",
    "\n",
    "        super(MinecraftActorCriticPolicy, self).__init__(\n",
    "            observation_space, action_space, lr_schedule, **kwargs\n",
    "        )\n",
    "\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def forward(\n",
    "        self, observation: Dict[str, torch.Tensor], deterministic: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "\n",
    "        # unpack observation\n",
    "        obs, first, state_in = self.unpack_dict_obs(observation)\n",
    "\n",
    "        # inference\n",
    "        (pi_logits, vpred, _), state_out = self.minerl_agent.policy(\n",
    "            obs, first, state_in\n",
    "        )\n",
    "\n",
    "        # update MineRLAgent's hidden state (important: only do this in forward()!)\n",
    "        self.minerl_agent.hidden_state = state_out\n",
    "\n",
    "        # action sampling\n",
    "        action = self.action_net.sample(pi_logits, deterministic=deterministic)\n",
    "\n",
    "        value = self.value_net.denormalize(vpred)[:, 0]\n",
    "        log_prob = self.action_net.logprob(action, pi_logits)\n",
    "\n",
    "        # convert agent action into array so it can pass through the SB3 functions\n",
    "        array_action = torch.cat((action[\"camera\"], action[\"buttons\"]), dim=-1)\n",
    "\n",
    "        return array_action.squeeze(1), value, log_prob\n",
    "\n",
    "    def _build(self, lr_schedule: Schedule) -> None:\n",
    "        \"\"\"\n",
    "        Create the networks and the optimizer.\n",
    "        :param lr_schedule: Learning rate schedule\n",
    "            lr_schedule(1) is the initial learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup action and value heads\n",
    "        self.action_net = self.minerl_agent.policy.pi_head\n",
    "        self.value_net = self.minerl_agent.policy.value_head\n",
    "        print(self.minerl_agent)\n",
    "\n",
    "        # Setup optimizer with initial learning rate\n",
    "        self.optimizer = self.optimizer_class(\n",
    "            self.\n",
    "            minerl_agent.policy.pi_head\n",
    "            .parameters()\n",
    "            , lr=lr_schedule(1), **self.optimizer_kwargs\n",
    "        )\n",
    "\n",
    "    def evaluate_actions(\n",
    "        self, obs: Dict[str, torch.Tensor], actions: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert array actions to agent actions\n",
    "        agent_actions = {\"camera\": actions[..., 0], \"buttons\": actions[..., 1]}\n",
    "\n",
    "        # unpack observation\n",
    "        img_obs, first, state_in = self.unpack_dict_obs(obs)\n",
    "\n",
    "        # inference\n",
    "        (pi_logits, vpred, _), state_out = self.minerl_agent.policy(\n",
    "            img_obs, first, state_in\n",
    "        )\n",
    "\n",
    "        value = self.value_net.denormalize(vpred)[:, 0]\n",
    "        log_prob = self.action_net.logprob(agent_actions, pi_logits)\n",
    "        entropy = self.action_net.entropy(pi_logits)\n",
    "\n",
    "        return value, log_prob, entropy\n",
    "\n",
    "    def predict_values(self, obs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get the estimated values according to the current policy given the observations.\n",
    "        :param obs:\n",
    "        :return: the estimated values.\n",
    "        \"\"\"\n",
    "\n",
    "        # unpack observation\n",
    "        img_obs, first, state_in = self.unpack_dict_obs(obs)\n",
    "\n",
    "        # inference\n",
    "        (_, latent_vf), state_out = self.minerl_agent.policy.net(\n",
    "            img_obs, state_in, {\"first\": first}\n",
    "        )\n",
    "        value = self.value_net(latent_vf)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def get_distribution(self, obs: Dict[str, torch.Tensor]) -> Distribution:\n",
    "        \"\"\"\n",
    "        Get the current policy distribution given the observations.\n",
    "        :param obs:\n",
    "        :return: the action distribution.\n",
    "        \"\"\"\n",
    "        # unpack observation\n",
    "        img_obs, first, state_in = self.unpack_dict_obs(obs)\n",
    "\n",
    "        # inference\n",
    "        (latent_pi, _), state_out = self.minerl_agent.policy.net(\n",
    "            img_obs,\n",
    "            state_in,\n",
    "            {\"first\": first},\n",
    "        )\n",
    "        # features = self.extract_features(obs)\n",
    "        # latent_pi = self.mlp_extractor.forward_actor(features)\n",
    "        return self._get_action_dist_from_latent(latent_pi)\n",
    "\n",
    "    def _get_action_dist_from_latent(self, latent_pi: torch.Tensor) -> Distribution:\n",
    "        \"\"\"\n",
    "        Retrieve action distribution given the latent codes.\n",
    "        :param latent_pi: Latent code for the actor\n",
    "        :return: Action distribution\n",
    "        \"\"\"\n",
    "        mean_actions = self.action_net(latent_pi)\n",
    "        # convert mean agent actions to mean array actions\n",
    "        mean_array_actions = (\n",
    "            torch.cat((mean_actions[\"camera\"], mean_actions[\"buttons\"]), -1)\n",
    "            .squeeze(0)\n",
    "            .squeeze(0)\n",
    "        )\n",
    "\n",
    "        print(self.action_dist, mean_array_actions.shape)\n",
    "        if isinstance(self.action_dist, MultiCategoricalDistribution):\n",
    "            return self.action_dist.proba_distribution(action_logits=mean_array_actions)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action distribution\")\n",
    "\n",
    "    def unpack_dict_obs(\n",
    "        self, obs: Dict[str, torch.Tensor]\n",
    "    ) -> Tuple[\n",
    "        Dict[str, torch.Tensor],\n",
    "        torch.Tensor,\n",
    "        List[Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Unpack the observation dictionary\n",
    "        :param obs:\n",
    "        :return: the agent image observation, first input tensor and the hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        img_obs = {\"img\": obs[\"img\"]}\n",
    "        first_obs = obs[\"first\"].bool()\n",
    "        state_in_obs = []\n",
    "\n",
    "        for i in range(len(self.minerl_agent.hidden_state)):\n",
    "            state_in1 = obs[\"state_in1\"][:, i, :, :]\n",
    "            if torch.isnan(state_in1).all():\n",
    "                state_in1 = None\n",
    "            else:\n",
    "                state_in1 = state_in1.bool()\n",
    "\n",
    "            state_in_tuple = (\n",
    "                state_in1,\n",
    "                (obs[\"state_in2\"][:, i, :, :], obs[\"state_in3\"][:, i, :, :]),\n",
    "            )\n",
    "            state_in_obs.append(state_in_tuple)\n",
    "\n",
    "        return img_obs, first_obs, \n",
    "\n",
    "policy_kwargs = dict(minerl_agent=agent)  # **agent_kwargs)\n",
    "# ppo = PPO(\"CnnPolicy\", venv, verbose=1, callback=WandbCallback())\n",
    "# ppo = PPO(\"CnnPolicy\", venv, verbose=1, batch_size=4, n_steps=8)\n",
    "ppo = PPO(MinecraftActorCriticPolicy, venv, verbose=1, policy_kwargs=policy_kwargs,\n",
    "          batch_size=4, n_steps=8)\n",
    "ppo.learn(total_timesteps=25000)\n",
    "\n",
    "# print(\"---Launching Minetest enviroment---\")\n",
    "# obs = minetest_to_minerl_obs(env.reset())\n",
    "# done = False\n",
    "# while not done:\n",
    "#     minerl_action = agent.get_action(obs)\n",
    "#     minetest_action = minerl_to_minetest_action(minerl_action, env)\n",
    "#     obs, reward, done, info = env.step(minetest_action)\n",
    "#     obs = minetest_to_minerl_obs(obs)\n",
    "#     if show:\n",
    "#         env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "964ce28d-b0a5-4267-9318-2b28d2c79426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minetest: no process found\n",
      "---Loading model---\n",
      "recurrence transformer\n",
      "Dict(BACKWARD:Discrete(2), DIG:Discrete(2), DROP:Discrete(2), FORWARD:Discrete(2), HOTBAR_NEXT:Discrete(2), HOTBAR_PREV:Discrete(2), INVENTORY:Discrete(2), JUMP:Discrete(2), LEFT:Discrete(2), MIDDLE:Discrete(2), MOUSE:Box([-1024  -600], [1024  600], (2,), int64), PLACE:Discrete(2), RIGHT:Discrete(2), SLOT_1:Discrete(2), SLOT_2:Discrete(2), SLOT_3:Discrete(2), SLOT_4:Discrete(2), SLOT_5:Discrete(2), SLOT_6:Discrete(2), SLOT_7:Discrete(2), SLOT_8:Discrete(2), SNEAK:Discrete(2))\n",
      "Dict(BACKWARD:Discrete(2), DIG:Discrete(2), DROP:Discrete(2), FORWARD:Discrete(2), HOTBAR_NEXT:Discrete(2), HOTBAR_PREV:Discrete(2), INVENTORY:Discrete(2), JUMP:Discrete(2), LEFT:Discrete(2), MIDDLE:Discrete(2), MOUSE:Box([-1024  -600], [1024  600], (2,), int64), PLACE:Discrete(2), RIGHT:Discrete(2), SLOT_1:Discrete(2), SLOT_2:Discrete(2), SLOT_3:Discrete(2), SLOT_4:Discrete(2), SLOT_5:Discrete(2), SLOT_6:Discrete(2), SLOT_7:Discrete(2), SLOT_8:Discrete(2), SNEAK:Discrete(2))\n",
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "<agent.MineRLAgent object at 0x7fb6603eed00>\n",
      "tensor([[[[140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          ...,\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140]],\n",
      "\n",
      "         [[186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          ...,\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186]],\n",
      "\n",
      "         [[250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          ...,\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250]]],\n",
      "\n",
      "\n",
      "        [[[140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          ...,\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140],\n",
      "          [140, 140, 140,  ..., 140, 140, 140]],\n",
      "\n",
      "         [[186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          ...,\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186],\n",
      "          [186, 186, 186,  ..., 186, 186, 186]],\n",
      "\n",
      "         [[250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          ...,\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250],\n",
      "          [250, 250, 250,  ..., 250, 250, 250]]]], device='cuda:0',\n",
      "       dtype=torch.uint8)\n",
      "Traceback (most recent call last):\n",
      "  File \"stab.py\", line 428, in <module>\n",
      "    ppo.learn(total_timesteps=25000)\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py\", line 317, in learn\n",
      "    return super().learn(\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 262, in learn\n",
      "    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 172, in collect_rollouts\n",
      "    actions, values, log_probs = self.policy(obs_tensor)\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"stab.py\", line 263, in forward\n",
      "    obs, first, state_in = self.unpack_dict_obs(observation)\n",
      "  File \"stab.py\", line 404, in unpack_dict_obs\n",
      "    img_obs = {\"img\": obs[\"img\"]}\n",
      "TypeError: new(): invalid data type 'str'\n"
     ]
    }
   ],
   "source": [
    "!killall minetest\n",
    "!python stab.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
