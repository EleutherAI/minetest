{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7120aaa-4083-4b42-9ef9-696782ebd1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install stable-baselines3\n",
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30299731-ef99-4e99-b556-7bbfa362f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vpt-minetest' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rk1a/vpt-minetest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d03423d-4f8c-41c7-a51c-4027d64e4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!wget -c https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-2x.weights\n",
    "!wget -c https://openaipublic.blob.core.windows.net/minecraft-rl/models/2x.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "279cd3e2-3402-4cd0-b23a-102f032a8271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stab.py\n"
     ]
    }
   ],
   "source": [
    "%%file stab.py\n",
    "import pickle\n",
    "import numpy as np\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from gym.spaces import Box, Discrete, MultiDiscrete\n",
    "from gym.wrappers import Monitor, TimeLimit\n",
    "from minetest_env import Minetest\n",
    "from stable_baselines3 import PPO\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from stable_baselines3.common.distributions import (\n",
    "    Distribution,\n",
    "    MultiCategoricalDistribution,\n",
    ")\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "import torch\n",
    "import gym\n",
    "import gc\n",
    "import sys\n",
    "if \"./vpt-minetest\" not in sys.path:\n",
    "    sys.path.append(\"./vpt-minetest\")\n",
    "from agent import MineRLAgent\n",
    "from run_vpt_agent import minetest_to_minerl_obs, minerl_to_minetest_action\n",
    "\n",
    "\n",
    "model, weights, video_dir, minetest_path, max_steps, show, seed, show_agent_pov = \"2x.model\", \"foundation-model-2x.weights\", \"videos\", \"../bin/minetest\", 100000, False, 32, False\n",
    "\n",
    "\n",
    "class DiscreteActions(gym.ActionWrapper):\n",
    "    def __init__(self, env, discretes=27):\n",
    "        self.env = env\n",
    "        self.discretes = discretes\n",
    "        sizes = []\n",
    "        self.vals = []\n",
    "        for i, v in env.action_space.spaces.items():\n",
    "            self.vals.append(len(sizes))\n",
    "            if isinstance(v, Discrete):\n",
    "                sizes.append(v.n)\n",
    "            elif isinstance(v, Box):\n",
    "                for _ in v.low:\n",
    "                    sizes.append(discretes)\n",
    "                \n",
    "        self.action_space = MultiDiscrete(sizes)  # TODO\n",
    "    \n",
    "    def action(self, act):\n",
    "        return {k: (np.asarray(act[i:i+len(v.low)]) / self.discretes * (v.high - v.low) + v.low\n",
    "                    if isinstance(v, Box) else act[i]).astype(v.dtype)\n",
    "                for i, (k, v) in zip(self.vals, self.env.action_space.spaces.items())}\n",
    "\n",
    "def make_env(\n",
    "    minetest_path: str,\n",
    "    rank: int,\n",
    "    seed: int = 0,\n",
    "    max_steps: int = 1e9,\n",
    "    env_kwargs: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    env_kwargs = env_kwargs or {}\n",
    "\n",
    "    def _init():\n",
    "        # Make sure that each Minetest instance has\n",
    "        # - different server and client ports\n",
    "        # - different and deterministic seeds\n",
    "        env = Minetest(\n",
    "            env_port=5555 + rank,\n",
    "            server_port=30000 + rank,\n",
    "            # seed=seed + rank,\n",
    "            world_dir=f\"../worlds/myworld{rank}\",\n",
    "            minetest_executable=minetest_path,\n",
    "            # xvfb_headless=False,\n",
    "            config_path=\"../minetest.conf\",\n",
    "            **env_kwargs,\n",
    "        )\n",
    "        env.reset_world = True\n",
    "        env = TimeLimit(env, max_episode_steps=max_steps)\n",
    "        env = DiscreteActions(env)\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "# Env settings\n",
    "seed = 42\n",
    "max_steps = 1000\n",
    "env_kwargs = {\"display_size\": (1024, 600), \"fov\": 72}\n",
    "\n",
    "# Create a vectorized environment\n",
    "num_envs = 2  # Number of envs to use (<= number of avail. cpus)\n",
    "# vec_env_cls = SubprocVecEnv\n",
    "vec_env_cls = DummyVecEnv\n",
    "venv = vec_env_cls(\n",
    "    [\n",
    "        make_env(minetest_path=minetest_path, rank=i, seed=seed, max_steps=max_steps, env_kwargs=env_kwargs)\n",
    "        for i in range(num_envs)\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"---Loading model---\")\n",
    "agent_parameters = pickle.load(open(model, \"rb\"))\n",
    "policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "agent = MineRLAgent(\n",
    "    venv,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    pi_head_kwargs=pi_head_kwargs,\n",
    "    show_agent_perspective=show_agent_pov,\n",
    ")\n",
    "# agent.load_weights(weights)\n",
    "agent_kwargs = dict(\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    pi_head_kwargs=pi_head_kwargs,\n",
    "    show_agent_pov=show_agent_pov,\n",
    ")\n",
    "\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "class MinecraftActorCriticPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy class for actor-critic algorithms wrapping OpenAI's VPT models.\n",
    "    Used by A2C, PPO and the likes.\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param minerl_agent: MineRL agent to be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        # minerl_agent: MineRLAgent,\n",
    "        policy_kwargs: Dict = {},\n",
    "        pi_head_kwargs: Dict = {},\n",
    "        show_agent_pov: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        # self.minerl_agent = minerl_agent\n",
    "        print(policy_kwargs)\n",
    "        self.minerl_agent = MineRLAgent(\n",
    "            None,\n",
    "            # venv,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            pi_head_kwargs=pi_head_kwargs,\n",
    "            show_agent_perspective=show_agent_pov,\n",
    "        )\n",
    "\n",
    "        super(MinecraftActorCriticPolicy, self).__init__(\n",
    "            observation_space, action_space, lr_schedule, **kwargs\n",
    "        )\n",
    "\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def forward(\n",
    "        self, observation: Dict[str, torch.Tensor], deterministic: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "\n",
    "        # unpack observation\n",
    "        obs, first, state_in = self.unpack_dict_obs(observation)\n",
    "\n",
    "        # inference\n",
    "        (pi_logits, vpred, _), state_out = self.minerl_agent.policy(\n",
    "            obs, first, state_in\n",
    "        )\n",
    "\n",
    "        # update MineRLAgent's hidden state (important: only do this in forward()!)\n",
    "        self.minerl_agent.hidden_state = state_out\n",
    "\n",
    "        # action sampling\n",
    "        action = self.action_net.sample(pi_logits, deterministic=deterministic)\n",
    "\n",
    "        value = self.value_net.denormalize(vpred)[:, 0]\n",
    "        log_prob = self.action_net.logprob(action, pi_logits)\n",
    "\n",
    "        # convert agent action into array so it can pass through the SB3 functions\n",
    "        array_action = torch.cat((action[\"camera\"], action[\"buttons\"]), dim=-1)\n",
    "\n",
    "        return array_action.squeeze(1), value, log_prob\n",
    "\n",
    "    def _build(self, lr_schedule: Schedule) -> None:\n",
    "        \"\"\"\n",
    "        Create the networks and the optimizer.\n",
    "        :param lr_schedule: Learning rate schedule\n",
    "            lr_schedule(1) is the initial learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup action and value heads\n",
    "        self.action_net = self.minerl_agent.policy.pi_head\n",
    "        self.value_net = self.minerl_agent.policy.value_head\n",
    "\n",
    "        # Setup optimizer with initial learning rate\n",
    "        self.optimizer = self.optimizer_class(\n",
    "            self.parameters(), lr=lr_schedule(1), **self.optimizer_kwargs\n",
    "        )\n",
    "\n",
    "    def evaluate_actions(\n",
    "        self, obs: Dict[str, torch.Tensor], actions: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert array actions to agent actions\n",
    "        agent_actions = {\"camera\": actions[..., 0], \"buttons\": actions[..., 1]}\n",
    "\n",
    "        # unpack observation\n",
    "        img_obs, first, state_in = self.unpack_dict_obs(obs)\n",
    "\n",
    "        # inference\n",
    "        (pi_logits, vpred, _), state_out = self.minerl_agent.policy(\n",
    "            img_obs, first, state_in\n",
    "        )\n",
    "\n",
    "        value = self.value_net.denormalize(vpred)[:, 0]\n",
    "        log_prob = self.action_net.logprob(agent_actions, pi_logits)\n",
    "        entropy = self.action_net.entropy(pi_logits)\n",
    "\n",
    "        return value, log_prob, entropy\n",
    "\n",
    "    def predict_values(self, obs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get the estimated values according to the current policy given the observations.\n",
    "        :param obs:\n",
    "        :return: the estimated values.\n",
    "        \"\"\"\n",
    "\n",
    "        # unpack observation\n",
    "        img_obs, first, state_in = self.unpack_dict_obs(obs)\n",
    "\n",
    "        # inference\n",
    "        (_, latent_vf), state_out = self.minerl_agent.policy.net(\n",
    "            img_obs, state_in, {\"first\": first}\n",
    "        )\n",
    "        value = self.value_net(latent_vf)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def get_distribution(self, obs: Dict[str, torch.Tensor]) -> Distribution:\n",
    "        \"\"\"\n",
    "        Get the current policy distribution given the observations.\n",
    "        :param obs:\n",
    "        :return: the action distribution.\n",
    "        \"\"\"\n",
    "        # unpack observation\n",
    "        img_obs, first, state_in = self.unpack_dict_obs(obs)\n",
    "\n",
    "        # inference\n",
    "        (latent_pi, _), state_out = self.minerl_agent.policy.net(\n",
    "            img_obs,\n",
    "            state_in,\n",
    "            {\"first\": first},\n",
    "        )\n",
    "        # features = self.extract_features(obs)\n",
    "        # latent_pi = self.mlp_extractor.forward_actor(features)\n",
    "        return self._get_action_dist_from_latent(latent_pi)\n",
    "\n",
    "    def _get_action_dist_from_latent(self, latent_pi: torch.Tensor) -> Distribution:\n",
    "        \"\"\"\n",
    "        Retrieve action distribution given the latent codes.\n",
    "        :param latent_pi: Latent code for the actor\n",
    "        :return: Action distribution\n",
    "        \"\"\"\n",
    "        mean_actions = self.action_net(latent_pi)\n",
    "        # convert mean agent actions to mean array actions\n",
    "        mean_array_actions = (\n",
    "            torch.cat((mean_actions[\"camera\"], mean_actions[\"buttons\"]), -1)\n",
    "            .squeeze(0)\n",
    "            .squeeze(0)\n",
    "        )\n",
    "\n",
    "        if isinstance(self.action_dist, MultiCategoricalDistribution):\n",
    "            return self.action_dist.proba_distribution(action_logits=mean_array_actions)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action distribution\")\n",
    "\n",
    "    def unpack_dict_obs(\n",
    "        self, obs: Dict[str, torch.Tensor]\n",
    "    ) -> Tuple[\n",
    "        Dict[str, torch.Tensor],\n",
    "        torch.Tensor,\n",
    "        List[Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Unpack the observation dictionary\n",
    "        :param obs:\n",
    "        :return: the agent image observation, first input tensor and the hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        img_obs = {\"img\": obs[\"img\"]}\n",
    "        first_obs = obs[\"first\"].bool()\n",
    "        state_in_obs = []\n",
    "\n",
    "        for i in range(len(self.minerl_agent.hidden_state)):\n",
    "            state_in1 = obs[\"state_in1\"][:, i, :, :]\n",
    "            if torch.isnan(state_in1).all():\n",
    "                state_in1 = None\n",
    "            else:\n",
    "                state_in1 = state_in1.bool()\n",
    "\n",
    "            state_in_tuple = (\n",
    "                state_in1,\n",
    "                (obs[\"state_in2\"][:, i, :, :], obs[\"state_in3\"][:, i, :, :]),\n",
    "            )\n",
    "            state_in_obs.append(state_in_tuple)\n",
    "\n",
    "        return img_obs, first_obs, \n",
    "\n",
    "policy_kwargs = dict(**agent_kwargs)\n",
    "# ppo = PPO(\"CnnPolicy\", venv, verbose=1, callback=WandbCallback())\n",
    "ppo = PPO(\"CnnPolicy\", venv, verbose=1)\n",
    "# ppo = PPO(MinecraftActorCriticPolicy, venv, verbose=1, policy_kwargs=policy_kwargs)\n",
    "ppo.learn(total_timesteps=25000)\n",
    "\n",
    "# print(\"---Launching Minetest enviroment---\")\n",
    "# obs = minetest_to_minerl_obs(env.reset())\n",
    "# done = False\n",
    "# while not done:\n",
    "#     minerl_action = agent.get_action(obs)\n",
    "#     minetest_action = minerl_to_minetest_action(minerl_action, env)\n",
    "#     obs, reward, done, info = env.step(minetest_action)\n",
    "#     obs = minetest_to_minerl_obs(obs)\n",
    "#     if show:\n",
    "#         env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "964ce28d-b0a5-4267-9318-2b28d2c79426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minetest: no process found\n",
      "---Loading model---\n",
      "recurrence transformer\n",
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"stab.py\", line 322, in <module>\n",
      "    ppo.learn(total_timesteps=25000)\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py\", line 317, in learn\n",
      "    return super().learn(\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 246, in learn\n",
      "    total_timesteps, callback = self._setup_learn(\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 489, in _setup_learn\n",
      "    self._last_obs = self.env.reset()  # pytype: disable=annotation-type-mismatch\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_transpose.py\", line 110, in reset\n",
      "    return self.transpose_observations(self.venv.reset())\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\", line 63, in reset\n",
      "    obs = self.envs[env_idx].reset()\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/gym/core.py\", line 346, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/home/ax37/anaconda3/lib/python3.8/site-packages/gym/wrappers/time_limit.py\", line 27, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/home/ax37/code/cpp/mtst/hacking_testing/minetest_env.py\", line 384, in reset\n",
      "    byte_obs = self.socket.recv()\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 809, in zmq.backend.cython.socket.Socket.recv\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 845, in zmq.backend.cython.socket.Socket.recv\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 194, in zmq.backend.cython.socket._recv_copy\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!killall minetest\n",
    "!python stab.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
